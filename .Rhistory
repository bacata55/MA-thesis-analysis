?mutate
submit()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
bye()
mymatrix <- matrix(c(1,2,3,4), nrow=2, ncol=2)
mymatrix
myarray <- array(0, c(1,2,3))
myarray
?array
?cbind
install.packages(xtable)
install.packages("xtable")
install.packages("foreign")
?foreign
library(foreign)
?foreign
help(foreign)
q()
install.packages("quanteda")
## be sure to install the latest version from GitHub, using dev branch:
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
## and quantedaData
devtools::install_github("quantedaData", username="kbenoit")
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
install.packages("quanteda")
install.packages("quantedaData")
install.packages("devtools")
library(devtools)
## be sure to install the latest version from GitHub, using dev branch:
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
## and quantedaData
devtools::install_github("quantedaData", username="kbenoit")
library(quanteda)
library(quantedaData)
sampletxt <- "The police with their policing strategy instituted a policy of general
iterations at the Data Science Institute."
#Let's tokenize
tokens<-tokenize(sampletxt)
?tokenize
tokens
tokens<-tokenize(sampletxt, removePunct=TRUE)
tokens
stems<-wordstem(tokens)
stems
data("SOTUCorpus")
head(SOTUCorpus)
speeches<-data.frame(SOTUCorpus$documents)
speeches
last_speech_text<-speeches$texts[230]
last_speech_text
?dfm
class(speeches$texts)
obama_dfm<-dfm(last_speech_text)
?stopwords
stopwords("english")
obama_dfm<-dfm(last_speech_text, ignoredFeatures = stopwords("english"))
?topfeatures
full_dfm<-dfm(speeches$texts, ignoredFeatures = stopwords("english"))
topfeatures(full_dfm)
?tfidf
weighted<-tfidf(full_dfm)
topfeatures(weighted)
normalized<-tfidf(full_dfm, normalize=TRUE)
topfeatures(normalized)
collocations(last_speech_text)
?collocations
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
install.packages(plotly)
install.packages("plotly")
library(quanteda)
library(quantedaData)
##load data
data(inaugCorpus)
tokens<-tokenize(inaugCorpus, removePunct=TRUE)
Tee<-lapply(tokens,  length )
Tee<-sum(unlist(Tee))
tokens
Tee
mydfm <- dfm(inaugCorpus)
M<-length(mydfm@Dimnames$features)
M
k<- 44
b<-.49
k * (Tee)^b
inaugCorpus$document$texts[1]
inaugCorpus$document$texts[57]
mydfm <- dfm(inaugCorpus)
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
mydfm <- dfm(inaugCorpus, ignoredFeatures=stopwords("english"))
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
# regression to check if slope is approx -1.0
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
topfeatures(mydfm, 20)
last_speech_text<-inaugCorpus$document$texts[57]
last_speech_text
collocations(last_speech_text)
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
kwic(inaugCorpus, "terror", 3)
kwic(inaugCorpus, "angels", 3)
kwic(inaugCorpus, "slavery", 3)
x<-c(1,2,3)
y<-c(1,2,3)
##define the norm
norm_vec <- function(x) sqrt(sum(x^2))
norm_vec
norm_vec(x)
x %*% y / (norm_vec(x)*norm_vec(y))
a<-c(1,2,3)
b<-c(1,2,4000)
a %*% b / (norm_vec(a)*norm_vec(b))
last_speech_text<-inaugCorpus$document$texts[57]
first_speech_text<-inaugCorpus$document$texts[1]
inaug_dfm<-dfm(c(last_speech_text, first_speech_text),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
tmp
as.matrix(tmp)
inaug_dfm<-dfm(c(last_speech_text, first_speech_text))
#calculate similarity
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
inaug_dfm<-dfm(subset(inaugCorpus , Year > 1980),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
similarity(inaug_dfm, "2009-Obama", n = 5, margin = "documents")
install.packages("zelig")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("cem")
install.packages(c("curl", "NLP", "topicmodels"))
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("MatchIt")
install.packags("cem")
install.packages("cem")
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(MatchIt)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
setwd("/Users/lesliehuang/Dropbox/PS4/")
setwd("Users/lesliehuang/Dropbox/PS4")
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
class(df)
str(df)
class(tokens)
tokenz<-lapply(tokens,  length )
tokens
tokens[1]
tokenz[1]
library(MatchIt)
library(Zelig)
install.packages("graph")
typez<-lapply(lapply(tokens,  unique ), length)
typez[1]
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
## Lexical diversity measures
# TTR
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
tokenz<-lapply(tokens,  length )
typez<-lapply(lapply(tokens,  unique ), length)
TTRz<-mapply("/",typez,tokenz,SIMPLIFY = FALSE)
TTRz
df$ttr<-unlist(TTRz)
str(df)
plot(df$ttr)
library("Zelig", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
remove.packages("Zelig")
remove.packages("cem")
df$year<-as.numeric(df$year)
aggregate(df$ttr, by=list(df$year), FUN=mean)
table(df$year)
aggregate(df$ttr, by=list(df$party), FUN=mean)
table(df$party)
?readability
df$read_FRE<-readability(df$texts, "Flesch")
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
readability(df$texts, "Fucks")
df$read_FRE<-readability(df$texts, "Flesch")
df$read_FRE
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$read_DC<-readability(df$texts, "Dale.Chall")
aggregate(df$read_DC, by=list(df$year), FUN=mean)
aggregate(df$read_DC, by=list(df$party), FUN=mean)
read<-readability(df$texts)
cor(read$Flesch, read$Dale.Chall)
TTRz
cor(read$Flesch, read$SMOG)
cor(read$Coleman.Liau, read$Dale.Chall)
cor(read$Fucks, read$Dale.Chall)
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
year_FRE<-data.frame(matrix(ncol = 5, nrow = 100))
df<-filter(df, party != "WUAG" & party != "SOC"  & party != "PBPA" )
party_FRE<-data.frame(matrix(ncol = 6, nrow = 100))
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
library(quanteda)
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
colnames(year_FRE)<-names(table(df$year))
colnames(party_FRE)<-names(table(df$party))
std <- function(x) sd(x)/sqrt(length(x))
year_ses<-apply(year_FRE, 2, std)
year_means<-apply(year_FRE, 2, mean)
party_ses<-apply(party_FRE, 2, std)
party_means<-apply(party_FRE, 2, mean)
###Plot results--year
coefs<-year_means
ses<-year_ses
y.axis <- c(1:5)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(year_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
library(ggplot2)
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
table(df$year)
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
coefs<-party_means
ses<-party_ses
y.axis <- c(1:6)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(party_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
##real world data
table(df$party)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$
browseVignettes(package = "dplyr")
0.1*9
# Leslie Huang
rm(list=ls())
setwd("/Users/lesliehuang/Dropbox/MA-thesis-analysis")
libraries <- c("foreign", "utils", "dplyr", "devtools")
lapply(libraries, require, character.only=TRUE)
# read in the statement CSVs
years <- c(2012, 2013, 2014, 2015, 2016)
for (i in 1:length(years)) {
print(years[i])
}
# Leslie Huang
rm(list=ls())
setwd("/Users/lesliehuang/Dropbox/MA-thesis-analysis")
libraries <- c("foreign", "utils", "dplyr", "devtools")
lapply(libraries, require, character.only=TRUE)
# read in the statement CSVs
years <- c(2012, 2013, 2014, 2015, 2016)
for (i in 1:length(years)) {
filename <- paste(years[i], "govtstatements.csv", sep = "")
print(filename)
}
# Leslie Huang
rm(list=ls())
setwd("/Users/lesliehuang/Dropbox/MA-thesis-analysis")
libraries <- c("foreign", "utils", "dplyr", "devtools")
lapply(libraries, require, character.only=TRUE)
# read in the statement CSVs
statements <- read.csv("2012govtstatements.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015, 2016)
for (i in 1:length(years)) {
filename <- paste(years[i], "govtstatements.csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- cbind(statements, csv)
}
# read in the statement CSVs
statements <- read.csv("../MA-datasets/2012govtstatements.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015, 2016)
for (i in 1:length(years)) {
filename <- paste(years[i], "../MA-datasets/govtstatements.csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- cbind(statements, csv)
}
statements <- read.csv("../MA-datasets/2012govtstatements.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015, 2016)
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/", years[i], "govtstatements.csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- cbind(statements, csv)
}
paste("../MA-datasets/", years[1], "govtstatements.csv", sep = "")
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/", years[i], "govtstatements.csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- cbind(statements, csv)
}
statements <- read.csv("../MA-datasets/govtstatements2012.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015, 2016)
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/govtstatements", years[i], ".csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- cbind(statements, csv)
}
statements <- read.csv("../MA-datasets/govtstatements2012.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015, 2016)
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/govtstatements", years[i], ".csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- rbind(statements, csv)
}
years <- c(2013, 2014, 2015)
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/govtstatements", years[i], ".csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- rbind(statements, csv)
}
View(statements)
View(statements)
2016statements <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
2016statements["text"]
2016statements <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
2016statements <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
2016 <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
statements2016 <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
statements2016$text
statements2016$URL <- NA
View(statements2016)
statements <- rbind(statements, statements2016)
View(statements)
FARC <- read.csv("../MA-datasets/FARC_communiques.csv", stringsAsFactors = FALSE)
View(FARC)
View(FARC)
View(statements)
View(statements)
write.csv(statements, file = "govtstatements.csv")
# Leslie Huang
# LIWC analysis of FARC communiques
rm(list=ls())
setwd("/Users/lesliehuang/Dropbox/MA/")
libraries <- c("foreign", "utils", "stargazer", "dplyr", "devtools", "quanteda", "quantedaData", "ggplot2", "stringr", "LIWCalike")
lapply(libraries, require, character.only=TRUE)
spanish_dict <- dictionary(file = "../LIWC/Spanish_LIWC2007_Dictionary.dic", format = "LIWC")
# import FARC communiques
FARC <- read.csv("../MA-datasets/FARC_communiques.csv", stringsAsFactors = FALSE)
# metadata: get date
FARC_meta <- select(FARC, date)
# create corpus
FARC_corp <- corpus(FARC$text, docvars = FARC_meta)
FARC_dfm <- dfm(FARC_corp, dictionary = spanish_dict)
output <- liwcalike(FARC$text, spanish_dict)
View(output)
View(output)
hist(output$EmoPos)
output$EmoPos
output$EmoNeg
class(output$EmoPos)
class(output$EmoPos[1])
View(output)
View(output)
mean(output$WC)
median(output$WC)
hist(output$WC)
hist(output$EmoPos)
