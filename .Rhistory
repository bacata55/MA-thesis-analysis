sTot <- sum((galton$child - mu) ^ 2)
sRes <- sum(deviance(sTot))
sRes <- deviant(sTot)
sRes <- deviance(sTot)
sRes <- deviance(galton$child)
sRes <- deviance(mu)
bye
sRes <- deviance(fit)
1 - sRes/sTot
summary(fit)$r.squared
cor(galton)^2
cor(galton$parent, galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent - 1, galton)
lm(child ~ parent, galton)
lm(child ~ 1, galton)
head(trees)
bye()
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
?select
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version =="3.1.1", country == "US")
?Comparison
filter(cran, country == "IN", r_version <= "3.0.2")
filter(cran, country == "US" | country =="IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3,5,NA,10))
!is.na(c(3,5,NA,10))
filter(cran, r_version == !is.na())
filter(cran, r_version == !is.na((3,5,NA,10)))
filter(cran, r_version == !is.na(r_version))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran, desc(ip_id))
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size/ 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by()
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package)
summarize(by_package, mean(size))
submit()
pack_sum
quantile(package_sum$count, probs = 0.99)
quantile(pack_sum$count, probs = 0.99)
filter(pack)sum, count > 679
filter(pack_sum, count > 679)
top_counts <- filter(pack_sum, count > 679)
top_counts
View(top_counts)
top_counts_sorted <- arrange(top_counts, desc(counts))
top_counts_sorted <- arrange(top_counts, desc(count()))
top_counts_sorted <- arrange(top_counts, desc(top_counts$count))
?arrange
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
submit()
submit()
submit()
submit()
submit()
?mutate
submit()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
bye()
mymatrix <- matrix(c(1,2,3,4), nrow=2, ncol=2)
mymatrix
myarray <- array(0, c(1,2,3))
myarray
?array
?cbind
install.packages(xtable)
install.packages("xtable")
install.packages("foreign")
?foreign
library(foreign)
?foreign
help(foreign)
q()
install.packages("quanteda")
## be sure to install the latest version from GitHub, using dev branch:
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
## and quantedaData
devtools::install_github("quantedaData", username="kbenoit")
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
install.packages("quanteda")
install.packages("quantedaData")
install.packages("devtools")
library(devtools)
## be sure to install the latest version from GitHub, using dev branch:
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
## and quantedaData
devtools::install_github("quantedaData", username="kbenoit")
library(quanteda)
library(quantedaData)
sampletxt <- "The police with their policing strategy instituted a policy of general
iterations at the Data Science Institute."
#Let's tokenize
tokens<-tokenize(sampletxt)
?tokenize
tokens
tokens<-tokenize(sampletxt, removePunct=TRUE)
tokens
stems<-wordstem(tokens)
stems
data("SOTUCorpus")
head(SOTUCorpus)
speeches<-data.frame(SOTUCorpus$documents)
speeches
last_speech_text<-speeches$texts[230]
last_speech_text
?dfm
class(speeches$texts)
obama_dfm<-dfm(last_speech_text)
?stopwords
stopwords("english")
obama_dfm<-dfm(last_speech_text, ignoredFeatures = stopwords("english"))
?topfeatures
full_dfm<-dfm(speeches$texts, ignoredFeatures = stopwords("english"))
topfeatures(full_dfm)
?tfidf
weighted<-tfidf(full_dfm)
topfeatures(weighted)
normalized<-tfidf(full_dfm, normalize=TRUE)
topfeatures(normalized)
collocations(last_speech_text)
?collocations
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
install.packages(plotly)
install.packages("plotly")
library(quanteda)
library(quantedaData)
##load data
data(inaugCorpus)
tokens<-tokenize(inaugCorpus, removePunct=TRUE)
Tee<-lapply(tokens,  length )
Tee<-sum(unlist(Tee))
tokens
Tee
mydfm <- dfm(inaugCorpus)
M<-length(mydfm@Dimnames$features)
M
k<- 44
b<-.49
k * (Tee)^b
inaugCorpus$document$texts[1]
inaugCorpus$document$texts[57]
mydfm <- dfm(inaugCorpus)
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
mydfm <- dfm(inaugCorpus, ignoredFeatures=stopwords("english"))
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
# regression to check if slope is approx -1.0
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
topfeatures(mydfm, 20)
last_speech_text<-inaugCorpus$document$texts[57]
last_speech_text
collocations(last_speech_text)
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
kwic(inaugCorpus, "terror", 3)
kwic(inaugCorpus, "angels", 3)
kwic(inaugCorpus, "slavery", 3)
x<-c(1,2,3)
y<-c(1,2,3)
##define the norm
norm_vec <- function(x) sqrt(sum(x^2))
norm_vec
norm_vec(x)
x %*% y / (norm_vec(x)*norm_vec(y))
a<-c(1,2,3)
b<-c(1,2,4000)
a %*% b / (norm_vec(a)*norm_vec(b))
last_speech_text<-inaugCorpus$document$texts[57]
first_speech_text<-inaugCorpus$document$texts[1]
inaug_dfm<-dfm(c(last_speech_text, first_speech_text),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
tmp
as.matrix(tmp)
inaug_dfm<-dfm(c(last_speech_text, first_speech_text))
#calculate similarity
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
inaug_dfm<-dfm(subset(inaugCorpus , Year > 1980),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
similarity(inaug_dfm, "2009-Obama", n = 5, margin = "documents")
install.packages("zelig")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("cem")
install.packages(c("curl", "NLP", "topicmodels"))
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("MatchIt")
install.packags("cem")
install.packages("cem")
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(MatchIt)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
setwd("/Users/lesliehuang/Dropbox/PS4/")
setwd("Users/lesliehuang/Dropbox/PS4")
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
class(df)
str(df)
class(tokens)
tokenz<-lapply(tokens,  length )
tokens
tokens[1]
tokenz[1]
library(MatchIt)
library(Zelig)
install.packages("graph")
typez<-lapply(lapply(tokens,  unique ), length)
typez[1]
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
## Lexical diversity measures
# TTR
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
tokenz<-lapply(tokens,  length )
typez<-lapply(lapply(tokens,  unique ), length)
TTRz<-mapply("/",typez,tokenz,SIMPLIFY = FALSE)
TTRz
df$ttr<-unlist(TTRz)
str(df)
plot(df$ttr)
library("Zelig", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
remove.packages("Zelig")
remove.packages("cem")
df$year<-as.numeric(df$year)
aggregate(df$ttr, by=list(df$year), FUN=mean)
table(df$year)
aggregate(df$ttr, by=list(df$party), FUN=mean)
table(df$party)
?readability
df$read_FRE<-readability(df$texts, "Flesch")
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
readability(df$texts, "Fucks")
df$read_FRE<-readability(df$texts, "Flesch")
df$read_FRE
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$read_DC<-readability(df$texts, "Dale.Chall")
aggregate(df$read_DC, by=list(df$year), FUN=mean)
aggregate(df$read_DC, by=list(df$party), FUN=mean)
read<-readability(df$texts)
cor(read$Flesch, read$Dale.Chall)
TTRz
cor(read$Flesch, read$SMOG)
cor(read$Coleman.Liau, read$Dale.Chall)
cor(read$Fucks, read$Dale.Chall)
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
year_FRE<-data.frame(matrix(ncol = 5, nrow = 100))
df<-filter(df, party != "WUAG" & party != "SOC"  & party != "PBPA" )
party_FRE<-data.frame(matrix(ncol = 6, nrow = 100))
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
library(quanteda)
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
colnames(year_FRE)<-names(table(df$year))
colnames(party_FRE)<-names(table(df$party))
std <- function(x) sd(x)/sqrt(length(x))
year_ses<-apply(year_FRE, 2, std)
year_means<-apply(year_FRE, 2, mean)
party_ses<-apply(party_FRE, 2, std)
party_means<-apply(party_FRE, 2, mean)
###Plot results--year
coefs<-year_means
ses<-year_ses
y.axis <- c(1:5)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(year_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
library(ggplot2)
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
table(df$year)
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
coefs<-party_means
ses<-party_ses
y.axis <- c(1:6)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(party_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
##real world data
table(df$party)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$
browseVignettes(package = "dplyr")
0.1*9
rm(list=ls())
setwd("/Users/lesliehuang/Dropbox/MA-thesis-analysis/")
libraries <- c("foreign", "utils", "stargazer", "dplyr", "devtools", "quanteda", "quantedaData", "ggplot2", "stringr", "LIWCalike", "topicmodels", "lda", "stm", "LDAvis", "austin", "forecast", "lmtest", "strucchange", "vars", "tseries", "urca")
lapply(libraries, require, character.only=TRUE)
# get LIWC dict
spanish_dict <- dictionary(file = "../LIWC/Spanish_LIWC2007_Dictionary.dic", format = "LIWC")
# some major dates for plotting
major_violence <- as.Date(c("7/20/13", "1/16/13", "7/29/14", "11/16/14", "4/15/15", "5/31/15", "6/15/15", "6/22/15"), "%m/%d/%y")
major_agree <- as.Date(c("8/26/12", "5/26/13", "11/6/13", "5/16/14", "3/7/15", "6/2/15", "9/23/15"), "%m/%d/%y")
cf_start <- as.Date(c("11/20/12", "12/15/13", "5/16/14", "12/20/14", "7/20/15"), "%m/%d/%y")
cf_end <- as.Date(c("1/20/13", "1/15/14", "5/28/14", "5/22/15", "1/1/16"), "%m/%d/%y")
ceasefires <- data.frame(start = as.Date(c("11/20/12", "12/15/13", "5/16/14", "12/20/14", "7/20/15"), "%m/%d/%y"), end = as.Date(c("1/20/13", "1/15/14", "5/28/14", "5/22/15", "1/1/16"), "%m/%d/%y"))
# dataframe of all dates
dates <- rbind(data.frame(date = major_violence, group = "major_viol"), data.frame(date = major_agree, group = "major_agree"))
################################################################################## import FARC communiques
FARC <- read.csv("../MA-datasets/FARC_communiques.csv", stringsAsFactors = FALSE)
# metadata: get date
FARC_meta <- select(FARC, date)
FARC_dates <- as.Date(FARC_meta[[1]], "%Y-%m-%d")
# run LIWC
liwc_FARC <- liwcalike(FARC$text, spanish_dict)
