mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by()
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package)
summarize(by_package, mean(size))
submit()
pack_sum
quantile(package_sum$count, probs = 0.99)
quantile(pack_sum$count, probs = 0.99)
filter(pack)sum, count > 679
filter(pack_sum, count > 679)
top_counts <- filter(pack_sum, count > 679)
top_counts
View(top_counts)
top_counts_sorted <- arrange(top_counts, desc(counts))
top_counts_sorted <- arrange(top_counts, desc(count()))
top_counts_sorted <- arrange(top_counts, desc(top_counts$count))
?arrange
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
submit()
submit()
submit()
submit()
submit()
?mutate
submit()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
bye()
mymatrix <- matrix(c(1,2,3,4), nrow=2, ncol=2)
mymatrix
myarray <- array(0, c(1,2,3))
myarray
?array
?cbind
install.packages(xtable)
install.packages("xtable")
install.packages("foreign")
?foreign
library(foreign)
?foreign
help(foreign)
q()
install.packages("quanteda")
## be sure to install the latest version from GitHub, using dev branch:
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
## and quantedaData
devtools::install_github("quantedaData", username="kbenoit")
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
install.packages("quanteda")
install.packages("quantedaData")
install.packages("devtools")
library(devtools)
## be sure to install the latest version from GitHub, using dev branch:
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
## and quantedaData
devtools::install_github("quantedaData", username="kbenoit")
library(quanteda)
library(quantedaData)
sampletxt <- "The police with their policing strategy instituted a policy of general
iterations at the Data Science Institute."
#Let's tokenize
tokens<-tokenize(sampletxt)
?tokenize
tokens
tokens<-tokenize(sampletxt, removePunct=TRUE)
tokens
stems<-wordstem(tokens)
stems
data("SOTUCorpus")
head(SOTUCorpus)
speeches<-data.frame(SOTUCorpus$documents)
speeches
last_speech_text<-speeches$texts[230]
last_speech_text
?dfm
class(speeches$texts)
obama_dfm<-dfm(last_speech_text)
?stopwords
stopwords("english")
obama_dfm<-dfm(last_speech_text, ignoredFeatures = stopwords("english"))
?topfeatures
full_dfm<-dfm(speeches$texts, ignoredFeatures = stopwords("english"))
topfeatures(full_dfm)
?tfidf
weighted<-tfidf(full_dfm)
topfeatures(weighted)
normalized<-tfidf(full_dfm, normalize=TRUE)
topfeatures(normalized)
collocations(last_speech_text)
?collocations
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
install.packages(plotly)
install.packages("plotly")
library(quanteda)
library(quantedaData)
##load data
data(inaugCorpus)
tokens<-tokenize(inaugCorpus, removePunct=TRUE)
Tee<-lapply(tokens,  length )
Tee<-sum(unlist(Tee))
tokens
Tee
mydfm <- dfm(inaugCorpus)
M<-length(mydfm@Dimnames$features)
M
k<- 44
b<-.49
k * (Tee)^b
inaugCorpus$document$texts[1]
inaugCorpus$document$texts[57]
mydfm <- dfm(inaugCorpus)
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
mydfm <- dfm(inaugCorpus, ignoredFeatures=stopwords("english"))
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
# regression to check if slope is approx -1.0
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
topfeatures(mydfm, 20)
last_speech_text<-inaugCorpus$document$texts[57]
last_speech_text
collocations(last_speech_text)
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
kwic(inaugCorpus, "terror", 3)
kwic(inaugCorpus, "angels", 3)
kwic(inaugCorpus, "slavery", 3)
x<-c(1,2,3)
y<-c(1,2,3)
##define the norm
norm_vec <- function(x) sqrt(sum(x^2))
norm_vec
norm_vec(x)
x %*% y / (norm_vec(x)*norm_vec(y))
a<-c(1,2,3)
b<-c(1,2,4000)
a %*% b / (norm_vec(a)*norm_vec(b))
last_speech_text<-inaugCorpus$document$texts[57]
first_speech_text<-inaugCorpus$document$texts[1]
inaug_dfm<-dfm(c(last_speech_text, first_speech_text),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
tmp
as.matrix(tmp)
inaug_dfm<-dfm(c(last_speech_text, first_speech_text))
#calculate similarity
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
inaug_dfm<-dfm(subset(inaugCorpus , Year > 1980),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
similarity(inaug_dfm, "2009-Obama", n = 5, margin = "documents")
install.packages("zelig")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("cem")
install.packages(c("curl", "NLP", "topicmodels"))
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("MatchIt")
install.packags("cem")
install.packages("cem")
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(MatchIt)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
setwd("/Users/lesliehuang/Dropbox/PS4/")
setwd("Users/lesliehuang/Dropbox/PS4")
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
class(df)
str(df)
class(tokens)
tokenz<-lapply(tokens,  length )
tokens
tokens[1]
tokenz[1]
library(MatchIt)
library(Zelig)
install.packages("graph")
typez<-lapply(lapply(tokens,  unique ), length)
typez[1]
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
## Lexical diversity measures
# TTR
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
tokenz<-lapply(tokens,  length )
typez<-lapply(lapply(tokens,  unique ), length)
TTRz<-mapply("/",typez,tokenz,SIMPLIFY = FALSE)
TTRz
df$ttr<-unlist(TTRz)
str(df)
plot(df$ttr)
library("Zelig", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
remove.packages("Zelig")
remove.packages("cem")
df$year<-as.numeric(df$year)
aggregate(df$ttr, by=list(df$year), FUN=mean)
table(df$year)
aggregate(df$ttr, by=list(df$party), FUN=mean)
table(df$party)
?readability
df$read_FRE<-readability(df$texts, "Flesch")
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
readability(df$texts, "Fucks")
df$read_FRE<-readability(df$texts, "Flesch")
df$read_FRE
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$read_DC<-readability(df$texts, "Dale.Chall")
aggregate(df$read_DC, by=list(df$year), FUN=mean)
aggregate(df$read_DC, by=list(df$party), FUN=mean)
read<-readability(df$texts)
cor(read$Flesch, read$Dale.Chall)
TTRz
cor(read$Flesch, read$SMOG)
cor(read$Coleman.Liau, read$Dale.Chall)
cor(read$Fucks, read$Dale.Chall)
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
year_FRE<-data.frame(matrix(ncol = 5, nrow = 100))
df<-filter(df, party != "WUAG" & party != "SOC"  & party != "PBPA" )
party_FRE<-data.frame(matrix(ncol = 6, nrow = 100))
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
library(quanteda)
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
colnames(year_FRE)<-names(table(df$year))
colnames(party_FRE)<-names(table(df$party))
std <- function(x) sd(x)/sqrt(length(x))
year_ses<-apply(year_FRE, 2, std)
year_means<-apply(year_FRE, 2, mean)
party_ses<-apply(party_FRE, 2, std)
party_means<-apply(party_FRE, 2, mean)
###Plot results--year
coefs<-year_means
ses<-year_ses
y.axis <- c(1:5)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(year_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
library(ggplot2)
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
table(df$year)
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
coefs<-party_means
ses<-party_ses
y.axis <- c(1:6)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(party_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
##real world data
table(df$party)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$
browseVignettes(package = "dplyr")
0.1*9
rm(list=ls())
setwd("/Users/lesliehuang/Dropbox/MA-thesis-analysis")
libraries <- c("foreign", "utils", "dplyr", "devtools")
lapply(libraries, require, character.only=TRUE)
# read in the statement CSVs
statements <- read.csv("../MA-datasets/govtstatements2012.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015)
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/govtstatements", years[i], ".csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- rbind(statements, csv)
}
statements2016 <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
statements2016$URL <- NA
statements <- rbind(statements, statements2016)
statements <- rbind(statements, statements2016)
statements <- read.csv("../MA-datasets/govtstatements2012.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015)
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/govtstatements", years[i], ".csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- rbind(statements, csv)
}
statements <- rbind(statements, statements2016)
statements <- rbind(statements, statements2016)
View(statements)
class(statements)
statements <- read.csv("../MA-datasets/govtstatements2012.csv", stringsAsFactors = FALSE)
years <- c(2013, 2014, 2015)
for (i in 1:length(years)) {
filename <- paste("../MA-datasets/govtstatements", years[i], ".csv", sep = "")
csv <- read.csv(filename, stringsAsFactors = FALSE)
statements <- rbind(statements, csv)
}
statements2016 <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
statements2016$URL <- NA
View(statements2016)
View(statements)
statements <- rbind(statements, statements2016)
View(statements)
View(statements)
s2012 <- read.csv("../MA-datasets/govtstatements2012.csv", stringsAsFactors = FALSE)
s2013 <- read.csv("../MA-datasets/govtstatements2013.csv", stringsAsFactors = FALSE)
s2014 <- read.csv("../MA-datasets/govtstatements2014.csv", stringsAsFactors = FALSE)
s2015 <- read.csv("../MA-datasets/govtstatements2015.csv", stringsAsFactors = FALSE)
s2016 <- read.csv("../MA-datasets/govtstatements2016.csv", stringsAsFactors = FALSE)
s2016$URL <- NA
View(statements)
View(statements)
rm(statements)
rm(statements2016)
View(csv)
rm(csv)
View(s2012)
s2012 <- s2012[-c(5, 6, 10, 15, 16, 17)]
View(s2012)
View(s2012)
s2012 <- s2012[-5]
s2012 <- slice(s2012, -c(5, 6, 10, 15, 16, 17))
View(s2012)
View(s2012)
View(s2013)
View(s2013)
s2013 <- slice(s2013, -c(2,3,8,10,11,15-21,23,29,32,36-38,41,43,45,47,48,52,65,66,69,70-74,79,80,82-85,90-92,95,99,101,102,105, 106,108,110,113,115,116,118,121,122,127,129,130,132,133,135-137,143-144,146,149-150,152,154-155,157,161,166-168,170,173,177,182,188,191))
s2013 <- slice(s2013, -c(2,3,8,10,11,15,16,17,18,19,20,21,23,29,32,36,37,38,41,43,45,47,48,52,65,66,69,70,71,72,73,74,79,80,82,83,84,85,90,91,92,95,99,101,102,105, 106,108,110,113,115,116,118,121,122,127,129,130,132,133,135,136,137,143,144,146,149,150,152,154,155,157,161,166,167,168,170,173,177,182,188,191))
s2014 <- slice(s2014, -c(6,8,9,10,11,13,17,28,39,41,42))
s2016 <- slice(s2016, -c(5,6,9,11,14,17,20,29,39,42,44,49))
View(s2014)
View(s2014)
View(s2016)
View(s2016)
View(s2012)
View(s2013)
s2014_2 <- read.csv("../MA-datasets/govtstatements2014_2.csv", stringsAsFactors = FALSE)
s2015 <- read.csv("../MA-datasets/govtstatements2015_1.csv", stringsAsFactors = FALSE)
S2014_2 <- slice(s2014_2, -c(3,4,8,16,21,25,26,32,33,37,39))
s2014_2 <- slice(s2014_2, -c(3,4,8,16,21,25,26,32,33,37,39))
s2015 <- slice(s2015, -c(2,4,5,6,15,20,21,23,25))
rm(S2014_2)
statements <- rbind(s2012, s2013, s2014, s2014_2, s2015, s2016)
write.csv(statements, file = "govtstatements.csv")
